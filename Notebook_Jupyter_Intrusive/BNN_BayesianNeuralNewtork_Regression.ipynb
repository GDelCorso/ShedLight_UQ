{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9934259-870f-4f18-9f45-726b8ca51a39",
   "metadata": {},
   "source": [
    "# Chapter: BNN (Bayesian Neural Network) Regression model\n",
    "This chapter aims to show how to define a fully bayesian model for regression and how to train it using Kullback-Leibler divergence in a vanilla Pytorch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f777bbad-4eb5-4c15-98de-e0c9988501d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "path_pardir = Path(os.getcwd()).parent\n",
    "path_data = os.path.join(path_pardir, 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6accaf-39ef-4d15-abfd-5e6111e63a41",
   "metadata": {},
   "source": [
    "## Data preparation and training/validation/test set preparation\n",
    "This is an elementary implementation of a multi-head regression model in Pytorch using the Variance Attenuation Loss.\n",
    "\n",
    "First we define the device (GPU/CPU) we are working with. Since we are working with simple numerical data, we choose the CPU to reduce the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcab84a-136e-4eb7-9cb4-535611f728de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the existing device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Select cpu device\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925012d6-711e-4334-9123-a18781e24752",
   "metadata": {},
   "source": [
    "The data frame is the linear relationship with homoscedastic aleatoric noise, we explicitly split the data frame into training/validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892bfb4e-df9f-472a-8845-892d7a68f8d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_range</th>\n",
       "      <th>y_range</th>\n",
       "      <th>split_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-1.827437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-99.89995</td>\n",
       "      <td>-2.038570</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-99.79990</td>\n",
       "      <td>-2.156036</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-99.69985</td>\n",
       "      <td>-1.881278</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-99.59980</td>\n",
       "      <td>-2.062530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-99.49975</td>\n",
       "      <td>-1.886184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-99.39970</td>\n",
       "      <td>-1.834947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-99.29965</td>\n",
       "      <td>-1.823319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_range   y_range  split_label\n",
       "0 -100.00000 -1.827437            0\n",
       "1  -99.89995 -2.038570            0\n",
       "2  -99.79990 -2.156036            2\n",
       "3  -99.69985 -1.881278            2\n",
       "4  -99.59980 -2.062530            0\n",
       "5  -99.49975 -1.886184            1\n",
       "6  -99.39970 -1.834947            0\n",
       "7  -99.29965 -1.823319            0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_df = \"df_nonlinear_homoscedastic_aleatoric.csv\"\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(path_data, chosen_df))\n",
    "\n",
    "training_df = full_df[full_df[\"split_label\"]==0]\n",
    "validation_df = full_df[full_df[\"split_label\"]==1]\n",
    "test_df = full_df[full_df[\"split_label\"]==2]\n",
    "\n",
    "full_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718eea0c-c1d5-446a-a4e8-64e81bf80dcb",
   "metadata": {},
   "source": [
    "We define a CustomDataset from Pytorch Dataset. The dataset has an *init* (initialise the class), a *len* and a *getitem* method.\n",
    "A Dataset must provide the tools to identify available data and return all the elements needed to train/test/evaluate the model.\n",
    "\n",
    "Can be skipped if the data is particularly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e5033d-40a8-40f2-b57b-58d41c744f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length training dataset: 1600\n",
      "Output example (x_range,y_range)=((tensor([-96.3982]), tensor([-1.8095]))) with index 27.\n"
     ]
    }
   ],
   "source": [
    "# Dataset definition:\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, path_data, split_label=None):\n",
    "        if split_label!=None:\n",
    "            self.df = pd.read_csv(os.path.join(path_data, annotations_file))\n",
    "            self.df = self.df[self.df['split_label']==split_label]\n",
    "        else:\n",
    "            self.df = pd.read_csv(os.path.join(path_data, annotations_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_range = float(self.df.iloc[idx]['x_range'])\n",
    "        y_range = float(self.df.iloc[idx]['y_range'])\n",
    "\n",
    "        features = torch.tensor([x_range], dtype=torch.float32)\n",
    "        output = torch.tensor([y_range], dtype=torch.float32)\n",
    "        \n",
    "        return features, output\n",
    "\n",
    "# Initialize Training dataset and show outputs and properties:\n",
    "training_dataset = CustomDataset(annotations_file=chosen_df, path_data=path_data, split_label=0)\n",
    "print(\"Length training dataset:\", training_dataset.__len__())\n",
    "\n",
    "index_example = 27\n",
    "print(f\"Output example (x_range,y_range)=({training_dataset.__getitem__(index_example)}) with index {index_example}.\")\n",
    "\n",
    "# Initialize validation and test datasets:\n",
    "validation_dataset = CustomDataset(annotations_file=chosen_df, path_data=path_data, split_label=1)\n",
    "test_dataset = CustomDataset(annotations_file=chosen_df, path_data=path_data, split_label=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc73af4-91f1-473b-9fbc-7480e82fd881",
   "metadata": {},
   "source": [
    "To define a batch we can iterate on the whole dataset or define a *DataLoader* which samples from the introduced DataSet and can be iterated during training. It provides a batch of tensors to apply the computations efficiently at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b673a6-f5d9-49b8-9064-4f5ce16d5d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Size of each batch\n",
    "batch_size = 15\n",
    "\n",
    "# Define 3 DataLoaders: Training/Validation/Test \n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e53fc0-b5d2-4ed7-916a-52c07d8ce58c",
   "metadata": {},
   "source": [
    "## Define a Bayesian Layer\n",
    "To define a fully Bayesian model for regression we need to define a single (linear) Bayesian layer.\n",
    "\n",
    "Each weight of the layer is made up of a *mu* (the mean) and a *log_sigma* (the logarithm of the standard deviation), which together give an output corresponding to a Gaussian distribution (fully characterised as $\\mathcal{N}\\left(\\textit{mu}, \\text{exp}\\left( \\textit{log\\_sigma} \\right) \\right)$).\n",
    "To ensure that the *sigma* is positive, the learnable value belongs to $\\mathbb{R}$ and corresponds to the logarithmic value of the standard deviation. Therefore, in the inference phase, we have to apply an exponential to make it correspond to the real standard deviation.\n",
    "\n",
    "The prior distribution of each distribution is introduced to update it using the KL divergence.\n",
    "\n",
    "The biases are computed in the same way (as Gaussian distributions) with their own prior distribution.\n",
    "\n",
    "The KL divergence is computed for the weights/biases of this layer and is added to all Bayesian layers to define the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b027db5-276d-4290-9427-ee0ca1db56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBayesianLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, prior_std=1.0):\n",
    "        '''\n",
    "        in_features: (int) number of input features\n",
    "        out_features: (int) number of output features\n",
    "        bias=Ture: (bool) add bias to mu and std\n",
    "        prior_std: (float>0) standard deviation of prior distributions\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.bias = bias\n",
    "\n",
    "        # Learnable mean and log-standard_deviation\n",
    "        # Define a (non-initialized) tensor with in_features to out_features dimensions\n",
    "        self.mu = torch.empty(out_features, in_features)  \n",
    "        # Random Gaussian initialization from values sampled from N(mean=0, std=0.1) \n",
    "        self.mu.normal_(0, 0.1)  \n",
    "        # Make it a trainable parameter\n",
    "        self.mu = nn.Parameter(self.mu)   \n",
    "\n",
    "        # Initialize a trainable parameter with in_features to out_features to -5\n",
    "        self.log_sigma = nn.Parameter(torch.Tensor(out_features, in_features).fill_(-5)) \n",
    "\n",
    "        # Fixed prior N(mean=0, std=prior_std) to compute KL divergence\n",
    "        self.prior_mu = torch.zeros_like(self.mu)\n",
    "        self.prior_sigma = torch.full_like(self.mu, prior_std)\n",
    "\n",
    "        \n",
    "        # The bias is another (distributional) trainable tensor\n",
    "        if self.bias:\n",
    "            # Initialize a learnable mean of bias\n",
    "            self.mu_bias = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
    "        \n",
    "            # Initialize a learnable std of bias\n",
    "            self.log_sigma_bias = nn.Parameter(torch.Tensor(out_features).fill_(-5))\n",
    "\n",
    "            # Fixed prior N(mean=0, std=prior_std) to compute bias KL divergence\n",
    "            self.prior_mu_bias = torch.zeros_like(self.mu_bias)\n",
    "            self.prior_sigma_bias = torch.full_like(self.mu_bias, prior_std)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, freezed = False):\n",
    "        '''\n",
    "        x: (tensor) input tensor\n",
    "        freezed=False: (bool) if True, freeze all the std and make the network determistic\n",
    "        \n",
    "        RETURN: (w*x + bias)\n",
    "        '''\n",
    "        \n",
    "        if freezed:\n",
    "            # Define each weight as the average value self.mu\n",
    "            weights = self.mu \n",
    "\n",
    "            if self.bias:\n",
    "                # Define each bias as the average bias\n",
    "                bias = self.mu_bias \n",
    "                \n",
    "        else:\n",
    "            # Sample weights using reparameterization trick: the weight is the N(mu, sigma) = mu + sigma*N(0,1)\n",
    "            epsilon_w = torch.randn_like(self.mu)       # Random N(0,1) sampling\n",
    "            sigma_w = torch.exp(self.log_sigma) + 1e-6  # Ensure positivity\n",
    "            weights = self.mu + sigma_w * epsilon_w     # Sampled weights\n",
    "\n",
    "            if self.bias:\n",
    "                # Sample bias using reparameterization trick\n",
    "                epsilon_b = torch.randn_like(self.mu_bias)       # Random N(0,1) sampling\n",
    "                sigma_b = torch.exp(self.log_sigma_bias) + 1e-6  # Ensure positivity\n",
    "                bias = self.mu_bias + sigma_b * epsilon_b        # Sampled bias\n",
    "\n",
    "        if self.bias:    \n",
    "            return F.linear(x, weights, bias)\n",
    "        else:\n",
    "            return F.linear(x, weights)\n",
    "\n",
    "    \n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        '''\n",
    "        Compute the KL divergence on each layer\n",
    "        '''\n",
    "        \n",
    "        # Get the device of model parameters\n",
    "        device = self.mu.device  \n",
    "\n",
    "        # Set up the weights prior distribution as normal (prior_mu, prior_sigma)\n",
    "        prior_w = torch.distributions.Normal(self.prior_mu.to(device), self.prior_sigma.to(device))\n",
    "        \n",
    "        # Set up the weights posterior distribution as normal (mu, sigma)\n",
    "        posterior_w = torch.distributions.Normal(self.mu.to(device), torch.exp(self.log_sigma).to(device))\n",
    "        \n",
    "        # Compute KL divergence for the weights of this layer\n",
    "        kl_w = torch.distributions.kl_divergence(posterior_w, prior_w).sum()  # KL for weights\n",
    "\n",
    "        \n",
    "        if self.bias:\n",
    "            # Set up the prior bias distribution as normal (prior_mu_bias, prior_sigma_bias)\n",
    "            prior_b = torch.distributions.Normal(self.prior_mu_bias.to(device), self.prior_sigma_bias.to(device))\n",
    "            \n",
    "            # Set up the posterior bias distribution as normal (mu_bias, sigma_bias)\n",
    "            posterior_b = torch.distributions.Normal(self.mu_bias.to(device), torch.exp(self.log_sigma_bias).to(device))\n",
    "\n",
    "        # Compute KL divergence for the bias of this layer\n",
    "        kl_b = torch.distributions.kl_divergence(posterior_b, prior_b).sum()  # KL for bias\n",
    "\n",
    "        if self.bias:\n",
    "            return kl_w + kl_b  # Sum both KL divergences\n",
    "        else:\n",
    "            return kl_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27626c-cf45-4a0d-8281-0d99f31c82f4",
   "metadata": {},
   "source": [
    "## Define a Bayesian model:\n",
    "We can now introduce the model, following the structure of the corresponding deterministic counterpart, by replacing each deterministic linear layer with a Bayesian one.\n",
    "\n",
    "We defined additional module methods to compute the KL divergence as an average of the KL over the layers, a *MC_forward* method to derive statistics from multiple model runs, and a *Frozen_forward* method to compute a (deterministic) output given all weights frozen to their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c608f33-d465-42c1-b0d6-0c96d3d8d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianModelRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):    \n",
    "        super().__init__()\n",
    "\n",
    "        # We store all the bayesian layers in a list to compute efficiently the KL divergence\n",
    "        self.bayesian_layers = nn.ModuleList([\n",
    "            LinearBayesianLayer(1, 8),\n",
    "            LinearBayesianLayer(8, 16),\n",
    "            LinearBayesianLayer(16, 32),\n",
    "            LinearBayesianLayer(32, 16),\n",
    "            LinearBayesianLayer(16, 16),\n",
    "            LinearBayesianLayer(16, 1)\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def forward(self, x, freezed = False):\n",
    "        '''\n",
    "        Define the sequential network by connecting LinearBayesianLayer with a ReLU activation function.\n",
    "\n",
    "        x: (tensor) input tensor of the model\n",
    "        freezed=False: (bool)\n",
    "\n",
    "        RETURN: x (tensor) prediction\n",
    "        '''\n",
    "        \n",
    "        for layer in self.bayesian_layers[:-1]:  \n",
    "            x = layer(x, freezed)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Last layer without activation function for regression\n",
    "        x = self.bayesian_layers[-1](x, freezed)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "    def MC_forward(self, x, n_iter=1000):\n",
    "        '''\n",
    "        Monte Carlo forward iterations.\n",
    "\n",
    "        To get the result of a BNN as a mean and a standard deviation, \n",
    "        we have to run the model several times and calculate statistics on the outputs.\n",
    "\n",
    "        x: (tensor) input tensor of the model\n",
    "        n_iter=1000: (int) number of iteratons to compute MC estimates\n",
    "\n",
    "        RETURN: mean_pred, std_pred (tuple of floats) \n",
    "        '''\n",
    "        \n",
    "        # Apply n_iter times the BNN model\n",
    "        predictions = torch.stack([self.forward(x) for _ in range(n_iter)], dim=0)  # Shape: [n_iter, batch_size, output_dim]\n",
    "\n",
    "        # Compute mean over MC iterations\n",
    "        mean_pred = predictions.mean(dim=0)  \n",
    "\n",
    "        # Compute standard deviation over MC iterations\n",
    "        std_pred = predictions.std(dim=0)    \n",
    "\n",
    "        # Return mean and std\n",
    "        return mean_pred, std_pred  \n",
    "\n",
    "    \n",
    "    def Frozen_forward(self, x):\n",
    "        '''\n",
    "        Freeze all the stds of the BNN and compute the (deterministic) output considering the weights as deterministic \n",
    "        values fixed to the means.\n",
    "\n",
    "        x: (tensor) input tensor of the model\n",
    "\n",
    "        RETURN: x (tensor) prediction\n",
    "        '''\n",
    "        \n",
    "        return self.forward(x, freezed = True)\n",
    "\n",
    "    \n",
    "    def kl_divergence(self):\n",
    "        '''\n",
    "        Average of the KL divergence of all Bayesian layers in the model.\n",
    "        '''\n",
    "        \n",
    "        return sum(layer.kl_divergence() for layer in self.bayesian_layers)/ len(self.bayesian_layers)\n",
    "\n",
    "\n",
    "# Initialize the model:\n",
    "model = BayesianModelRegression()\n",
    "\n",
    "# Move the model to device (GPU):\n",
    "model.to(device)\n",
    "\n",
    "# Convert weights type to float:\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1554392-3f91-4cb3-b28b-1dc426fe5657",
   "metadata": {},
   "source": [
    "Introduce the loss and the optimiser. Both can be implemented explicitly. \n",
    "\n",
    "For BNNs computed using Variational Inference (VI), the loss is defined as the standard MSE (on the prediction) plus the KL divergence (multiplied by a beta term) averaged over all Bayesian layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c67131-8bcd-498d-a39a-89d3f9e8ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function MSE (on prediction) + KL (on all Bayesian layers)\n",
    "def loss_BNN(y_pred, y_true, kl_div, beta=1e-5):\n",
    "\n",
    "    # Standard regression loss\n",
    "    mse = F.mse_loss(y_pred, y_true, reduction='mean')  \n",
    "\n",
    "    # Combine MSE with KL divergence\n",
    "    return mse + beta * kl_div  \n",
    "\n",
    "# Alternative loss where MSE (increase average predictions) plays a major role in the first epochs and then decreases (increase uncertainties understanding)\n",
    "def adaptive_loss_BNN(y_pred, y_true, kl_div, epoch, total_epochs):\n",
    "\n",
    "    # Increase over time\n",
    "    beta = (epoch / total_epochs) * 1e-3  \n",
    "\n",
    "    # Standard regression loss\n",
    "    mse = F.mse_loss(y_pred, y_true, reduction='mean')\n",
    "\n",
    "    # Combine MSE with KL divergence\n",
    "    return mse + beta * kl_div  \n",
    "   \n",
    "\n",
    "# Optimizer (Adam): \n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806bc0d-289d-4572-bf99-4719e97055d5",
   "metadata": {},
   "source": [
    "Training cycle: \n",
    "- The model is trained for a given number of epochs *n_epochs*\n",
    "- Each epoch iterates over the entire dataset (*shuffle == True* just randomly shuffles indices) using the dataloader to generate batches.\n",
    "- For each batch, the gradients are computed on the whole batch to the respect of the loss function and the optimizer it is used to update model trainable parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db53ec0-746f-4291-9aa7-e4dc263ae223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.8081, Val. Loss: 0.4749\n",
      "Epoch [2/30], Loss: 0.4359, Val. Loss: 0.3212\n",
      "Epoch [3/30], Loss: 0.2498, Val. Loss: 0.1721\n",
      "Epoch [4/30], Loss: 0.1387, Val. Loss: 0.1148\n",
      "Epoch [5/30], Loss: 0.1125, Val. Loss: 0.0941\n",
      "Epoch [6/30], Loss: 0.1045, Val. Loss: 0.0900\n",
      "Epoch [7/30], Loss: 0.0951, Val. Loss: 0.0809\n",
      "Epoch [8/30], Loss: 0.0921, Val. Loss: 0.0840\n",
      "Epoch [9/30], Loss: 0.0851, Val. Loss: 0.0855\n",
      "Epoch [10/30], Loss: 0.0831, Val. Loss: 0.0844\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs:\n",
    "n_epochs = 30\n",
    "\n",
    "training_loss_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "# Iterate over the epochs:\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Set the model to Training mode: This interacts with certain kind of network layers (such as Dropout layers)\n",
    "    model.train()  \n",
    "\n",
    "    # Temporary variable to store the loss on the whole epoch as a convergence metric\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate on the whole dataset using the dataloader.\n",
    "    for x_input, y_output in training_dataloader:\n",
    "        # Load inputs ad move to device (GPU)\n",
    "        x_input, y_output = x_input.to(device), y_output.to(device)\n",
    "\n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # Forward pass (model calls)\n",
    "        y_model = model(x_input)  \n",
    "        y_model = y_model.to(device)\n",
    "        \n",
    "        # Get total KL divergence\n",
    "        kl_div = model.kl_divergence()  \n",
    "        kl_div = kl_div.to(device)\n",
    "        \n",
    "        # Compute loss (supervised case)\n",
    "        loss = loss_BNN(y_model, y_output, kl_div)\n",
    "\n",
    "        # Backpropagation \n",
    "        loss.backward()  \n",
    "\n",
    "        # Update parameters (optimization step)\n",
    "        optimizer.step()  \n",
    "\n",
    "        # Update running loss as convergence metric\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "\n",
    "    \n",
    "    ## Calculate loss on validation as an additional metric to evaluate overfitting\n",
    "    # Set the model to Evaluation mode:\n",
    "    model.eval()\n",
    "\n",
    "    # Temporary variable to store the validation loss:\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    # Deactivate gradient computation\n",
    "    with torch.no_grad():\n",
    "        for x_input, y_output in validation_dataloader:\n",
    "            # Load inputs ad move to device (GPU)\n",
    "            x_input, y_output = x_input.to(device), y_output.to(device)\n",
    "\n",
    "            # Forward pass (model calls)\n",
    "            y_model = model(x_input)\n",
    "            y_model = y_model.to(device)\n",
    "            \n",
    "            # Get total KL divergence\n",
    "            kl_div = model.kl_divergence() \n",
    "            kl_div = kl_div.to(device)\n",
    "        \n",
    "            # Compute loss (supervised case)\n",
    "            loss = loss_BNN(y_model, y_output, kl_div)\n",
    "\n",
    "            # Update validation running loss as convergence metric\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "    # Average epoch loss\n",
    "    epoch_training_loss = running_loss / len(training_dataloader)\n",
    "    epoch_validation_loss = running_val_loss / len(validation_dataloader)\n",
    "\n",
    "    # Append the losses to the list:\n",
    "    training_loss_list.append(epoch_training_loss)\n",
    "    validation_loss_list.append(epoch_validation_loss)\n",
    "            \n",
    "    # Convergence metric\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_training_loss:.4f}, Val. Loss: {epoch_validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265da7e-d418-43c0-99de-2e71e0ca7453",
   "metadata": {},
   "source": [
    "Plot the convergence graphs on training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea287a97-5c3e-4547-811a-ccd7d48386a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(list(range(len(training_loss_list))), training_loss_list, color = 'black', label='Training loss')\n",
    "ax.plot(list(range(len(validation_loss_list))), validation_loss_list,  color = 'red', label='Validation loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f38ba-19c0-482d-aa89-86fb4c02e7a0",
   "metadata": {},
   "source": [
    "## Show predictions\n",
    "Plot the training data set against the predictions using the Frozen_forward method. The network will therefore produce outputs taking into account the weights equal to the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93a0cd-2b63-4c6f-b5ce-2937eed9c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model to evaluation mode:\n",
    "model.eval()\n",
    "\n",
    "# Define the dataframe \n",
    "x_plot = np.array(training_df[\"x_range\"])  # Input\n",
    "y_plot = np.array(training_df[\"y_range\"])  # Ground Truth\n",
    "\n",
    "# Get predictions using Frozen_forward method\n",
    "with torch.no_grad():\n",
    "    prediction_plot = [float(model.Frozen_forward(torch.tensor(i).view(1).to(device).float().unsqueeze(1))[0].to('cpu')) for i in x_plot]\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_plot_train = pd.DataFrame({\n",
    "    \"x\": np.append(x_plot, x_plot),\n",
    "    \"y\": np.append(y_plot, prediction_plot),\n",
    "    \"GT/Pred\": np.append([\"GT\"] * len(y_plot), [\"Pred\"] * len(prediction_plot))\n",
    "})\n",
    "\n",
    "# Scatter plot of GT and predictions\n",
    "sns.scatterplot(data=df_plot_train, x=\"x\", y=\"y\", hue=\"GT/Pred\", alpha=0.3)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a703d-e322-4b84-806f-f2007fcbc615",
   "metadata": {},
   "source": [
    "Standard predictions are inherently random, as each (stochastic) forward pass sample is taken randomly from the weight distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3dd0b7-fc6f-4409-a1b6-bb4c9e0248eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe \n",
    "x_plot = np.array(training_df[\"x_range\"])  # Input\n",
    "y_plot = np.array(training_df[\"y_range\"])  # Ground Truth\n",
    "\n",
    "# Set up the model to evaluation mode:\n",
    "model.eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    prediction_plot = [float(model(torch.tensor(i).view(1).to(device).float().unsqueeze(1))[0].to('cpu')) for i in x_plot]\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_plot_train = pd.DataFrame({\n",
    "    \"x\": np.append(x_plot, x_plot),\n",
    "    \"y\": np.append(y_plot, prediction_plot),\n",
    "    \"GT/Pred\": np.append([\"GT\"] * len(y_plot), [\"Pred\"] * len(prediction_plot))\n",
    "})\n",
    "\n",
    "# Scatter plot of GT and predictions\n",
    "sns.scatterplot(data=df_plot_train, x=\"x\", y=\"y\", hue=\"GT/Pred\", alpha=0.3)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37655a21-ed3d-48be-8c99-8a1c32cbb196",
   "metadata": {},
   "source": [
    "Collect the MC_forward predictions (calculated as mean and std) and plot them to identify the confidence intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f2807-6a41-4d79-992d-a8f08eea8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of iterations at each MC_forward pass\n",
    "tmp_iter = 50\n",
    "\n",
    "# Set up the model to evaluation mode:\n",
    "model.eval()\n",
    "\n",
    "# Define the dataframe \n",
    "x_plot = np.array(training_df[\"x_range\"])  # Input\n",
    "y_plot = np.array(training_df[\"y_range\"])  # GT\n",
    "\n",
    "# Select a subset (10% of the training set) to speed up computation\n",
    "x_plot_sampled = x_plot[::10]  # Take every 10th element from x_plot\n",
    "y_plot_sampled = y_plot[::10]  # Take every 10th element from y_plot\n",
    "\n",
    "# Compute model outputs - Computationally burdensome\n",
    "with torch.no_grad():\n",
    "    tuple_predictions = [model.MC_forward(torch.tensor(i).view(1).to(device).float(), n_iter = tmp_iter) for i in x_plot]\n",
    "\n",
    "# Divide outputs as predictions and stds:\n",
    "prediction_plot = [i[0].to('cpu').item()  for i in tuple_predictions]\n",
    "std_plot = [i[1].to('cpu').item()  for i in tuple_predictions]\n",
    "\n",
    "# Define the dataframe\n",
    "df_plot_train = pd.DataFrame({\n",
    "    \"x\": np.append(x_plot, x_plot),\n",
    "    \"y\": np.append(y_plot, prediction_plot),\n",
    "    \"GT/Pred\": np.append([\"GT\"]*len(y_plot), [\"Pred\"]*len(prediction_plot))\n",
    "})\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(data=df_plot_train, x=\"x\", y=\"y\", hue=\"GT/Pred\", alpha=0.07)\n",
    "\n",
    "# Create the prediction bounds for the color zone (±2 std)\n",
    "lower_bound = np.array(prediction_plot) - 2 * np.array(std_plot)\n",
    "upper_bound = np.array(prediction_plot) + 2 * np.array(std_plot)\n",
    "\n",
    "# Add the shaded region between lower_bound and upper_bound\n",
    "plt.fill_between(x_plot, lower_bound, upper_bound, color='orange', alpha=0.4, label=\"Uncertainty (±2 std)\")\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction with Uncertainty')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702275bd-c4ae-4afe-9dda-22fb6f6d22b8",
   "metadata": {},
   "source": [
    "As expected, most of the uncertainty is concentrated around the boundaries of the training set, while the aleatoric (noise) component in the central part of the set is neglected.\n",
    "This is an example of how (one-headed) BNNs are able to identify epistemic uncertainty and are less prone to correctly characterise aleatoric uncertainty.\n",
    "\n",
    "In summary, given an input value *x_prova*: \n",
    "- the *forward* pass provides a (stochastic) random value sampled from the weights\n",
    "- the *MC_forward* computes the forward pass several times and collects the sample mean and standard deviation\n",
    "- The *Frozen_forward* computes the deterministic output of the network, using the mean weights as the deterministic ANN counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880d8b1-09fc-40bf-8e01-f97d2229ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the test point\n",
    "x_prova = x_plot[1500]\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # One (stochastic) forward pass\n",
    "    print(\"\\n Standard forward: \", float(model(torch.tensor(x_prova).view(1).to(device).float().unsqueeze(1))[0].to('cpu')))\n",
    "    \n",
    "    # One average Monte Carlo forward pass obtained by applying n_iter times the (stochastic) forward pass\n",
    "    print(\"\\n MC_forward: \",model.MC_forward(torch.tensor(x_prova).view(1).to(device).float(), n_iter=1000))\n",
    "    \n",
    "    # One (determistic) forward pass using the average weights\n",
    "    print(\"\\n Frozen_forward: \", model.Frozen_forward(torch.tensor(x_prova).view(1).to(device).float()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
